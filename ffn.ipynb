{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d95f79b",
   "metadata": {},
   "source": [
    "# Predicting if stock's value will go UP or DOWN with Feed Forward Network\n",
    "The data is arranged in the following way:\n",
    "\n",
    "X: stock's value for 50 consecutive days\n",
    "\n",
    "y: 1 if stock's value on 51st day is higher than on 50th day, or 0 if it is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015f38a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/AAPL_2006-01-01_to_2018-01-01.csv\n",
      "Days:  3019\n",
      "X:  (2968, 50)\n",
      "y:  (2968, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime(2006, 1, 1)\n",
    "end = datetime.datetime(2018, 1, 1)\n",
    "start_date_str = str(start.date())\n",
    "end_date_str = str(end.date())\n",
    "\n",
    "#how many consecutive prices are in X\n",
    "sliding_window_size = 50\n",
    "\n",
    "\"\"\"\n",
    "stocks = ['MMM', 'AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS', 'XOM', 'GE',\n",
    "          'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PFE',\n",
    "          'PG', 'TRV', 'UTX', 'UNH', 'VZ', 'WMT', 'GOOGL', 'AMZN', 'AABA']\n",
    "\n",
    "\"\"\"\n",
    "stocks = ['AAPL']#, 'BA', 'CAT', 'CVX', 'CSCO', 'IBM']#, 'GOOGL', 'AMZN', 'AABA']\n",
    "\n",
    "data_x = np.empty([sliding_window_size])\n",
    "data_y = np.empty([1])\n",
    "data_y_price = np.empty([1]) \n",
    "\n",
    "for stock in stocks:\n",
    "    file_name = 'data/' + stock + '_' + start_date_str + '_to_' + end_date_str + '.csv'\n",
    "    print(file_name)\n",
    "    frame = pd.read_csv(file_name)\n",
    "    \n",
    "    #get only closing stock prices\n",
    "    frame = frame['Close']\n",
    "    \n",
    "    frame =(frame.values).reshape(-1,1)\n",
    "    data = (normalize(frame, axis=0)).squeeze()\n",
    "    print(\"Days: \", data.size)\n",
    "    \n",
    "    \n",
    "    for i in range(data.size - sliding_window_size - 1):\n",
    "        data_sample_x = data[i:i+sliding_window_size]\n",
    "\n",
    "        #EITHER EXACT PRICE OR INCREASE/DECREASE FORMAT\n",
    "        data_sample_y_price = data[i+sliding_window_size]\n",
    "        data_sample_y = 1 if data[i+sliding_window_size] > data[i+sliding_window_size - 1] else 0\n",
    "        data_x = np.vstack([data_x , data_sample_x])\n",
    "        data_y = np.vstack([data_y , data_sample_y])\n",
    "        data_y_price = np.vstack([data_y_price , data_sample_y_price])\n",
    "#remove 1st element, which was created by np.empty\n",
    "data_x = data_x[1:]\n",
    "data_y = data_y[1:]\n",
    "data_y_price = data_y_price[1:]\n",
    "\n",
    "print(\"X: \", data_x.shape)\n",
    "print(\"y: \", data_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509576c",
   "metadata": {},
   "source": [
    "### Split data into training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae374d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_samples = data_x.shape[0]\n",
    "\n",
    "validation_first_index = int(number_of_samples * 0.7)\n",
    "testing_first_index = int(number_of_samples * (0.7 + 0.15))\n",
    "\n",
    "data_x_train = data_x[:validation_first_index]\n",
    "data_y_train = data_y[:validation_first_index]\n",
    "\n",
    "data_x_val = data_x[validation_first_index:testing_first_index]\n",
    "data_y_val = data_y[validation_first_index:testing_first_index]\n",
    "\n",
    "data_x_test = data_x[testing_first_index:]\n",
    "data_y_test = data_y[testing_first_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269f904",
   "metadata": {},
   "source": [
    "### Transform data to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b785de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X:  torch.Size([2077, 50])\n",
      "Training y:  torch.Size([2077, 1])\n",
      "Validation X:  torch.Size([445, 50])\n",
      "Validation y:  torch.Size([445, 1])\n",
      "Testing X:  torch.Size([446, 50])\n",
      "Testing y:  torch.Size([446, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_tensors = Variable(torch.Tensor(data_x_train))\n",
    "X_val_tensors = Variable(torch.Tensor(data_x_val))\n",
    "X_test_tensors = Variable(torch.Tensor(data_x_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(data_y_train))\n",
    "y_val_tensors = Variable(torch.Tensor(data_y_val)) \n",
    "y_test_tensors = Variable(torch.Tensor(data_y_test)) \n",
    "\n",
    "print(\"Training X: \", X_train_tensors.shape)\n",
    "print(\"Training y: \", y_train_tensors.shape)\n",
    "print(\"Validation X: \", X_val_tensors.shape)\n",
    "print(\"Validation y: \", y_val_tensors.shape)\n",
    "print(\"Testing X: \", X_test_tensors.shape)\n",
    "print(\"Testing y: \", y_test_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19912c",
   "metadata": {},
   "source": [
    "### Change dimensions to (rows, timestamps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4ed2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: torch.Size([2077, 1, 50]) torch.Size([2077, 1])\n",
      "Validation Shape: torch.Size([445, 1, 50]) torch.Size([445, 1])\n",
      "Testing Shape: torch.Size([446, 1, 50]) torch.Size([446, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "\n",
    "X_val_tensors_final = torch.reshape(X_val_tensors,   (X_val_tensors.shape[0], 1, X_val_tensors.shape[1]))\n",
    "\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) \n",
    "\n",
    "print(\"Training Shape:\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
    "print(\"Validation Shape:\", X_val_tensors_final.shape, y_val_tensors.shape)\n",
    "print(\"Testing Shape:\", X_test_tensors_final.shape, y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85c12c",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Network with 2 Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a04d280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(FFN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, num_classes),\n",
    "    )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32cf2e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cea524f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, training loss: 0.72991, validation loss: 0.70027\n",
      "Epoch: 100, training loss: 0.34249, validation loss: 0.27916\n",
      "Epoch: 200, training loss: 0.25740, validation loss: 0.29673\n",
      "Epoch: 300, training loss: 0.25511, validation loss: 0.29138\n",
      "Epoch: 400, training loss: 0.25329, validation loss: 0.27855\n",
      "Epoch: 500, training loss: 0.25186, validation loss: 0.26845\n",
      "Epoch: 600, training loss: 0.25085, validation loss: 0.26118\n",
      "Epoch: 700, training loss: 0.25023, validation loss: 0.25625\n",
      "Epoch: 800, training loss: 0.24988, validation loss: 0.25328\n",
      "Epoch: 900, training loss: 0.24971, validation loss: 0.25165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 50 #number of features\n",
    "hidden_size = 10 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 #number of output classes \n",
    "\n",
    "ffn = FFN(num_classes, input_size)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ffn.parameters(), lr=learning_rate) \n",
    "\n",
    "lowest_loss = 999999\n",
    "for epoch in range(num_epochs):\n",
    "    output = ffn.forward(X_train_tensors_final) #forward pass\n",
    "    optimizer.zero_grad()\n",
    " \n",
    "    loss = criterion(output, y_train_tensors)\n",
    " \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    " \n",
    "    optimizer.step()\n",
    "    \n",
    "    #validation and early stopping\n",
    "    ffn.eval()\n",
    "    output = ffn(X_val_tensors_final)\n",
    "    val_loss = criterion(output, y_val_tensors)\n",
    "    if val_loss.item() < lowest_loss:\n",
    "        lowest_loss = val_loss.item()\n",
    "        torch.save(ffn.state_dict(), \"checkpoint-ffn.pt\")\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, training loss: %1.5f, validation loss: %1.5f\" % (epoch, loss, val_loss))\n",
    "\n",
    "    \n",
    "\n",
    "# load the last checkpoint with the best model\n",
    "ffn.load_state_dict(torch.load('checkpoint-ffn.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4db673",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab6a0b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness: 0.53587\n",
      "loss: 0.46413\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = torch.round(ffn.forward(X_test_tensors_final))\n",
    "    acc = torch.eq(output, y_test_tensors)\n",
    "    print(\"correctness: %1.5f\" % ((sum(acc)/ y_test_tensors.shape[0]).item()))\n",
    "    print(\"loss: %1.5f\" % (criterion(output, y_test_tensors)))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f92f97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(output, label='prediction')\n",
    "#plt.plot(y_test_tensors, label='real')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aee5ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_new = X_test_tensors_final[0:1]\\ny_test_tensors\\nprint(X_new.shape, X_test_tensors_final.shape, y_test_tensors.shape)\\noutput = []\\nwith torch.no_grad():\\n    for i in range(100):\\n        \\n        out = lstm1.forward(X_new)\\n        output.append(out)\\n        \\n        X_new = X_new[:, :, 1:50]\\n\\n        X_new = torch.cat((X_new, out.unsqueeze(1)), dim=2)\\n\\n\\ngood = 0\\nbad = 0\\nlast_pred = 0\\nlast_true = 0\\nfor pred_y, real_y in zip(output, y_test_tensors):\\n    if pred_y > last_pred and real_y > last_true:\\n        good+=1\\n    elif pred_y > last_pred and real_y < last_true:\\n        bad+=1\\n    elif pred_y < last_pred and real_y < last_true:\\n        good+=1\\n    elif pred_y < last_pred and real_y > last_true:\\n        bad+=1\\n    last_pred = pred_y\\n    last_true = real_y\\nprint(good, bad, output.shape, y_test_tensors)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generative mode\n",
    "\"\"\"\n",
    "X_new = X_test_tensors_final[0:1]\n",
    "y_test_tensors\n",
    "print(X_new.shape, X_test_tensors_final.shape, y_test_tensors.shape)\n",
    "output = []\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        \n",
    "        out = lstm1.forward(X_new)\n",
    "        output.append(out)\n",
    "        \n",
    "        X_new = X_new[:, :, 1:50]\n",
    "\n",
    "        X_new = torch.cat((X_new, out.unsqueeze(1)), dim=2)\n",
    "\n",
    "\n",
    "good = 0\n",
    "bad = 0\n",
    "last_pred = 0\n",
    "last_true = 0\n",
    "for pred_y, real_y in zip(output, y_test_tensors):\n",
    "    if pred_y > last_pred and real_y > last_true:\n",
    "        good+=1\n",
    "    elif pred_y > last_pred and real_y < last_true:\n",
    "        bad+=1\n",
    "    elif pred_y < last_pred and real_y < last_true:\n",
    "        good+=1\n",
    "    elif pred_y < last_pred and real_y > last_true:\n",
    "        bad+=1\n",
    "    last_pred = pred_y\n",
    "    last_true = real_y\n",
    "print(good, bad, output.shape, y_test_tensors)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303e327",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "Let's invest using our model and see if me make a profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fce40601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Money after investing: 1619.00\n"
     ]
    }
   ],
   "source": [
    "money = 1000\n",
    "\n",
    "#get prices matching the test dataset\n",
    "data_y_price_test = data_y_price[testing_first_index:]\n",
    "with torch.no_grad():\n",
    "    output = torch.round(ffn.forward(X_test_tensors_final))\n",
    "\n",
    "for i in range(1, data_y_price_test.shape[0]):\n",
    " \n",
    "    #if the model predicts that the price will rise then hold it\n",
    "    if output[i].item()==1:\n",
    "        money = money * data_y_price_test[i].item() / data_y_price_test[i-1].item()\n",
    "    #if the model predicts that the price will decrease then sell it and buy it the next day\n",
    "    else:\n",
    "        money = money * data_y_price_test[i-1].item() / data_y_price_test[i].item()\n",
    "\n",
    "print(\"Money after investing: %1.2f\" % money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f864385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len([i for i in output if i ==1]), len([i for i in output if i ==0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
